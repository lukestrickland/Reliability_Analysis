---
title: "Automation Failures in ATC: Standard Results"
author: "ljgs"
date: "20/11/2019"
output:
  word_document:
    reference_docx: ../assets/apa.docx
  pdf_document: default
  html_document: default
---
by Luke Strickland

```{r load_packages_and_data, echo= FALSE , results = "hide", message=FALSE, warning=FALSE}

source("dmc/dmc.R")
source("dmc/dmc_extras.R")
source("R/0-analysis_functions.R")
load_model ("LBA", "lba_B.R")

theme_set(theme_simple())

library(dplyr)
library(tidyr)
library(ggplot2)
library(pander)
library(snowfall)
options(digits=2)

fitpath <- file.path(set_fit_path(), "Reliability_Analysis")
loadpath <- create_loadpath(fitpath)
savepath <- create_savepath(fitpath)

loadpath("CA_top_samples_A_lb_final.RData")

CA_top_samples <- CA_top_samples_A_lb

# 
# pp <- h.post.predict.dmc(CA_top_samples, save.simulation = TRUE, cores=8)
# 
rescore_column <- function(df) {
  df$R <- factor(as.character(toupper(substr(df$S,1,1))==df$R))
  new_data <- attr(df, "data")
  new_data$R <- factor(as.character(toupper(substr(new_data$S,1,1))==new_data$R))
#  new_data <- new_data %>% select(C, everything())
  #%>% select(-R)
  attr(df, "data") <- new_data
#  df %>% select(reps, C, everything())
  #%>% select(-R)
  df
}
# 
# savepath(pp,
#      file="CA_top_samples_pp_A_lb.RData")

loadpath("CA_top_samples_pp_A_lb.RData")

pp1 <- lapply(pp, rescore_column)



tmp <- 1:24
tmp <- tmp[!tmp==4]
full_balance <- c(tmp, 28) 

CA_top_samples <- CA_top_samples[names(CA_top_samples) %in% full_balance]
pp1 <- pp1[names(pp1) %in% full_balance]
# 
# fitlist <- GET.fitgglist.dmc(pp1, factors=c("cond", "failtrial"))
# savepath(fitlist, file="fitlist_A_lb.RData")

loadpath("fitlist_A_lb.RData")
loadpath("postexp_summaries_A_lb.RData")
loadpath("model_correlations_A_lb.RData")


```

# Model Fit

We obtained Bayesian estimates of model parameters using the Dynamic Models of Choice R Suite (Heathcote et al., 2018). These estimates take the form of posterior distributions, which are proportional to probability distributions of the model parameters given the data and prior information about the parameter values. The details of estimation are discussed in the supplementary materials. Figure 5 displays fit of the posterior predictions of the model to the data. Generally, the model fitted the data well, including the effects of automation on accuracy and distributions of RT. There was some minor miss-fit to median correct RT in the high-reliability condition on automation-incorrect trials, with the model somewhat underestimating the slowing induced by the incorrect automated advice. However, the model did fit the direction of this effect, and that this represents a small number of responses (there are relatively few trials where the automation was incorrect in the high-reliability condition).

```{r echo= FALSE}
pp_cap <- "Figure 5. *Posterior predictions of performance, averaged over participants. The model predictions correspond to the white circles, the posterior means correspond to the black shaded dots. The error bars display the 95% posterior credible intervals of the predictions. Three quantiles of response time (RT) are depicted, with the 0.1 quantile of RT grouped on the bottom, the median RT at the middle, and the 0.9 quantile of RT at the top.*"
```

```{r accuracy_descriptives_fortext, echo= FALSE, message=FALSE, warning=FALSE, fig.height = 7, fig.width=7, fig.cap=pp_cap}



accs <- fitlist$pps %>% filter(R=="TRUE") %>% select(-R)

accs$cond <- factor(accs$cond, levels=c("M", "L", "H"), labels =
                      c("Manual", "Low Reliability", "High Reliability"))

accs$failtrial <- factor(accs$failtrial, levels=c("nonf", "fail"), labels =
                      c("Automation Correct", "Automation Incorrect"))

plot1 <- ggplot.RP.dmc(accs, xaxis="cond") +xlab("") +ylab("Accuracy")

corRTs <- fitlist$RTs %>% filter(R=="TRUE") %>% select(-R)

corRTs$cond <- factor(corRTs$cond, levels=c("M", "L", "H"), labels =
                      c("Manual", "Low Reliability", "High Reliability"))

corRTs$failtrial <- factor(corRTs$failtrial, levels=c("nonf", "fail"), labels =
                      c("Automation Correct", "Automation Incorrect"))

plot2 <- ggplot.RT.dmc(corRTs, xaxis="cond") +xlab("") +ylab("Correct RT")



errRTs <- fitlist$RTs %>% filter(R=="FALSE") %>% select(-R)

errRTs$cond <- factor(errRTs$cond, levels=c("M", "L", "H"), labels =
                      c("Manual", "Low Reliability", "High Reliability"))

errRTs$failtrial <- factor(errRTs$failtrial, levels=c("nonf", "fail"), labels =
                      c("Automation Correct", "Automation Incorrect"))

plot3 <- ggplot.RT.dmc(errRTs, xaxis="cond") +xlab("") +ylab("Error RT")


grid.arrange(plot1,plot2,plot3)


```

```{r evaluate_model_params_calc, echo= FALSE , results = "hide", message=FALSE, warning=FALSE}

# msds <-
#   get.msds(CA_top_samples[names(CA_top_samples) %in% full_balance])
# # 
# savepath(msds, file=
#       "msds_top_samples_balance_A_lb.RData")
# # 
# msds <-
#   get.msds(CA_top_samples[names(CA_top_samples) != "4"])
# 
# save(msds, file=
#       "samples_data/msds_top_samples.RData")
# 


loadpath("msds_top_A_lb.RData")

paste.msd <- function(x) paste(signif(x["M"],2), "(", 
                               signif(x["SD"],2), ")", sep="")
zpvec <- function(samples, fun){
    effect<- group.inference.dist(samples, fun)
    Z <- mean(effect)/sd(effect)
    p <- minp(effect)
    if(p<.001) p <- "< .001" else {
      p = round(p,3)
      p= paste("= .", 
      substr(p,3,10), sep="")
    }
    c(round(Z,2), p)
}
```


# Parameter Inference

For inference we created a group-averaged posterior distribution, by averaging the values of each posterior sample across participants. The values of the averaged model parameters are tabulated in the supplementary materials. In the following sections, we examine the effect of automation on accumulation rate and threshold parameters. To test parameter differences, we calculate a one-tailed posterior *p* value, corresponding to the proportion of posterior samples on which one parameter value was higher than another. To accord with the typical intuition associated with *p* values, we report the *p* value against whichever direction was closest to an observed effect (e.g., a *p* of 0 is evidence in favor of an effect). Many effects were ‘significant’ in the sense that *p* < .001. To give an estimate of effect size, we report the mean of the parameter differences divided by the standard deviation, referred to as *Z*.
 

# Excitation and Inhibition

Evidence accumulation rates are plotted in Figure 6. The effects of automation are evaluated by comparing the accumulation rate towards an automation trial with the corresponding accumulation rates in manual conditions. Excitation is indicated by increased accumulation towards the accumulator that agrees with the decision aid (i.e., match). For example, on a conflict trial on which the automation correctly recommends ‘conflict’, excitation would increase the conflict accumulation rate. Inhibition is indicated by reduced accumulation towards the accumulator that disagrees with the decision aid (i.e., mismatch). For example, for conflict trials on which the decision aid correctly labels a conflict, inhibition would reduce accumulation in the ‘non-conflict’ accumulator.

Table 2 contains statistical tests of excitation and inhibition effects. We found strong evidence of inhibition in both conditions, with accumulation rates lower to decisions disagreeing with automation than on matched manual trials, replicating Strickland et al. (2021). Further, inhibition was stronger in the high-reliability condition than in the low-reliability condition. Importantly, and in contrast to Strickland et al.’s results, in the high-reliability condition, we also found strong evidence of excitation on all trial types: accumulation rates were higher to decisions agreeing with automation than on matched manual trials.
 
 In the low-reliability condition, we did not find evidence of excitation on automation-correct trials. In fact, we found an effect in the opposite direction of excitation for low-reliability automation: non-conflict accumulation rates were actually lower on automation-correct trials than on matched manual trials. We did find some significant excitation effects on automation-incorrect trials. However, these effects were weaker than in the high-reliability condition, and automation-incorrect trials are relatively less well estimated than automation-correct trials because there are less of them. Thus, evidence for excitation in the low-reliability condition is less consistent than the high-reliability condition, and the detected effects are weaker. 


```{r echo= FALSE}
Vs_cap <- "Figure 6. Estimates of accumulation rates. The shapes indicate the posterior means and the error bars correspond to the mean plus or minus the posterior standard deviation. In cases where the shapes are overlapping,
the condition means are very close together (e.g., match, automation-correct, conflict, low reliability vs manual)."
```

```{r echo=FALSE, fig.cap=Vs_cap, fig.height=8, fig.width=9.425, message=FALSE, warning=FALSE, results="hide"}
Vs <- msds[grep("mean_v", rownames(msds)),]

Vs$Cond <- "Manual" 
Vs$Cond[grep("L", rownames(Vs))] <- "Low"
Vs$Cond[grep("H", rownames(Vs))] <- "High"
Vs$Auto <- "Automation Correct"
Vs$Auto[grep("fail", rownames(Vs))] <- "Automation Incorrect"
Vs$S <- "Conflict"
Vs$S[grep("nn", rownames(Vs))] <- "Non-conflict"
Vs$match <- "Match"
Vs$match[grep("false", rownames(Vs))] <- "Mismatch"

names(Vs)[names(Vs)=="Cond"] <- "Condition"

Vs$exinh <- NA
Vs$exinh[Vs$Auto=="Automation Correct" & Vs$match=="Match"] <- "Excitation"
Vs$exinh[Vs$Auto=="Automation Incorrect" & Vs$match=="Match"] <- "Inhibition"
Vs$exinh[Vs$Auto=="Automation Correct" & Vs$match=="Mismatch"] <- "Inhibition"
Vs$exinh[Vs$Auto=="Automation Incorrect" & Vs$match=="Mismatch"] <- "Excitation"


ggplot(Vs, aes(factor(Auto),M)) + 
  geom_point(stat = "identity",aes(shape=Condition, col=Condition), size=3) +
  geom_errorbar(aes(ymax = M + SD, ymin = M - SD, width = 0.3, col=Condition))+ 
  ylab("Accumulation Rate") + xlab("")+
  facet_grid(S ~ match,scales = "free", space = "fixed") +
    theme(text = element_text(size = 16))

```

*Table 2*. 
Statistical tests of automation-induced excitation and inhibition effects. We depict Z(p), where Z is the posterior mean of the parameter difference divided by its standard deviation, and p is the one-tailed posterior probability against their being an effect. L stands for low-reliability condition and H for the high-reliability condition. 

```{r echo= FALSE ,results = "hide", message=FALSE, warning=FALSE, results='asis'}

H_conf_inh_fail <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"mean_v.cc.M.fail.true",, drop=F] - 
           thetas[,"mean_v.cc.H.fail.true",, drop=F])

H_conf_ex_fail <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"mean_v.cc.H.fail.false",, drop=F] - 
          thetas[,"mean_v.cc.M.fail.false",, drop=F] 
           )

H_conf_ex_success <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"mean_v.cc.H.nonf.true",, drop=F] -
          thetas[,"mean_v.cc.M.nonf.true",, drop=F] 
           )

H_conf_inh_success <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"mean_v.cc.M.nonf.false",, drop=F] - 
           thetas[,"mean_v.cc.H.nonf.false",, drop=F])



H_nonconf_inh_fail <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"mean_v.nn.M.fail.true",, drop=F] - 
           thetas[,"mean_v.nn.H.fail.true",, drop=F])

H_nonconf_ex_fail <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"mean_v.nn.H.fail.false",, drop=F] - 
          thetas[,"mean_v.nn.M.fail.false",, drop=F] 
           )

H_nonconf_ex_success <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"mean_v.nn.H.nonf.true",, drop=F] -
          thetas[,"mean_v.nn.M.nonf.true",, drop=F] 
           )

H_nonconf_inh_success <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"mean_v.nn.M.nonf.false",, drop=F] - 
           thetas[,"mean_v.nn.H.nonf.false",, drop=F])



L_conf_inh_fail <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"mean_v.cc.M.fail.true",, drop=F] - 
           thetas[,"mean_v.cc.L.fail.true",, drop=F])

L_conf_ex_fail <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"mean_v.cc.L.fail.false",, drop=F] - 
          thetas[,"mean_v.cc.M.fail.false",, drop=F] 
           )

L_conf_ex_success <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"mean_v.cc.L.nonf.true",, drop=F] -
          thetas[,"mean_v.cc.M.nonf.true",, drop=F] 
           )

L_conf_inh_success <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"mean_v.cc.M.nonf.false",, drop=F] - 
           thetas[,"mean_v.cc.L.nonf.false",, drop=F])



L_nonconf_inh_fail <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"mean_v.nn.M.fail.true",, drop=F] - 
           thetas[,"mean_v.nn.L.fail.true",, drop=F])

L_nonconf_ex_fail <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"mean_v.nn.L.fail.false",, drop=F] - 
          thetas[,"mean_v.nn.M.fail.false",, drop=F] 
           )

L_nonconf_ex_success <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"mean_v.nn.L.nonf.true",, drop=F] -
          thetas[,"mean_v.nn.M.nonf.true",, drop=F] 
           )

L_nonconf_inh_success <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"mean_v.nn.M.nonf.false",, drop=F] - 
           thetas[,"mean_v.nn.L.nonf.false",, drop=F])





inhextab <- rbind(
  c("", "", "", ""),
  c(L_conf_ex_success, L_conf_inh_success, H_conf_ex_success, H_conf_inh_success),
  c(L_conf_ex_fail, L_conf_inh_fail, H_conf_ex_fail, H_conf_inh_fail),
  c("", "", "", ""),
  c(L_nonconf_ex_success, L_nonconf_inh_success, H_nonconf_ex_success, H_nonconf_inh_success),
  c(L_nonconf_ex_fail, L_nonconf_inh_fail, H_nonconf_ex_fail, H_nonconf_inh_fail)
  
      )

rownames(inhextab) <- c("Conflict Trials", "Automation Correct", "Automation Incorrect",
                        "Non-conflict Trials", "Automation Correct", "Automation Incorrect")
colnames(inhextab) <- c("Excitation (L)", "Inhibition (L)", "Excitation (H)", "Inhibition (H)")

pandoc.table(inhextab)


```


# Threshold effects

Threshold estimates are plotted in Figure 9. Statistical tests of differences across automated and manual conditions in thresholds are tabulated in Table 3. There were some small drops in threshold levels in the low-reliability condition compared with the manual condition, suggesting participants may have become less cautious when provided with automation. There were relatively stronger decreases in thresholds observed in the high-reliability condition when compared with the manual condition. 


```{r echo= FALSE}
Bs_cap <- "Figure 7. Estimates of thresholds. The shapes indicate the posterior means and the error bars correspond to the mean plus or minus the posterior standard deviation."
```

```{r echo= FALSE , results = "hide", message=FALSE, warning=FALSE, fig.cap = Bs_cap, fig.width = 8, fig.height=3.5}
Bs <- msds[grep("B", rownames(msds)),]

Bs$R <- "Non-conflict"
Bs$R[grep("C", rownames(Bs))] <- "Conflict"
Bs$Cond <- "Manual"
Bs$Cond[grep("L", rownames(Bs))] <- "Low"
Bs$Cond[grep("H", rownames(Bs))] <- "High"
Bs$Session <- "Session One"
Bs$Session[grep("2", rownames(Bs))] <- "Session Two"
Bs$Session[grep("3", rownames(Bs))] <- "Session Three"

Bs$Session <- factor(Bs$Session, levels=c("Session One", "Session Two", "Session Three"))


names(Bs)[names(Bs)=="Cond"] <- "Condition"

ggplot(Bs, aes(factor(R),M)) + 
  geom_point(stat = "identity",aes(col=Condition, shape=Condition), size=2.5) +
  geom_errorbar(aes(ymax = M + SD, ymin = M - SD, width = 0.3, col=Condition))+ 
  ylab("Threshold") + xlab("Accumulator")+
  facet_grid(.~Session) +
    theme(text = element_text(size = 14))


```

Table 3. 
*Statistical tests of differences in thresholds across automated and manual conditions. We depict Z(p), where Z is the posterior mean of the parameter difference divided by its standard deviation, and p is the one-tailed posterior probability against their being an effect.*

```{r echo= FALSE , results = "asis", message=FALSE, warning=FALSE, fig.width = 8, fig.height=6}

B_N_1_MvL <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"B.M.1.N",, drop=F] -
          thetas[,"B.L.1.N",, drop=F] 
           )

B_N_2_MvL <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"B.M.2.N",, drop=F] -
          thetas[,"B.L.2.N",, drop=F] 
           )

B_N_3_MvL <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"B.M.3.N",, drop=F] -
          thetas[,"B.L.3.N",, drop=F] 
           )



B_C_1_MvL <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"B.M.1.C",, drop=F] -
          thetas[,"B.L.1.C",, drop=F] 
           )

B_C_2_MvL <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"B.M.2.C",, drop=F] -
          thetas[,"B.L.2.C",, drop=F] 
           )

B_C_3_MvL <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"B.M.3.C",, drop=F] -
          thetas[,"B.L.3.C",, drop=F] 
           )



B_N_1_MvH <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"B.M.1.N",, drop=F] -
          thetas[,"B.H.1.N",, drop=F] 
           )

B_N_2_MvH <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"B.M.2.N",, drop=F] -
          thetas[,"B.H.2.N",, drop=F] 
           )

B_N_3_MvH <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"B.M.3.N",, drop=F] -
          thetas[,"B.H.3.N",, drop=F] 
           )



B_C_1_MvH <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"B.M.1.C",, drop=F] -
          thetas[,"B.H.1.C",, drop=F] 
           )

B_C_2_MvH <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"B.M.2.C",, drop=F] -
          thetas[,"B.H.2.C",, drop=F] 
           )

B_C_3_MvH <- zandp(CA_top_samples, 
        function (thetas)
          thetas[,"B.M.3.C",, drop=F] -
          thetas[,"B.H.3.C",, drop=F] 
           )


Btab <- rbind(
  c(B_C_1_MvL, B_C_2_MvL, B_C_3_MvL),
  c(B_N_1_MvL, B_N_2_MvL, B_N_3_MvL),
  c(B_C_1_MvH, B_C_2_MvH, B_C_3_MvH),
  c(B_N_1_MvH, B_N_2_MvH, B_N_3_MvH)

      )

rownames(Btab) <- c("Conflict Accumulator (L)",
                    "Non-conflict Accumulator (L)",
                    "Conflict Accumulator (H)",
                    "Non-conflict Accumulator (H)")
colnames(Btab) <- c("Session One", "Session Two", "Session Three")

pandoc.table(Btab)



```




# Posterior Exploration

To summarise, we found substantial evidence that both high- and low- reliability automation decision aids are incorporated into decision making with an inhibition mechanism, strong evidence that participants relied on an additional excitation mechanism for high-reliability automation, and weaker evidence of this mechanism for low-reliability automation. We also found some evidence that thresholds were reduced when automation was provided, particularly for high-reliability automation. Here, we conduct simulations to investigate the role of these mechanisms in explaining the observed effects of automation. We focus on automation costs and benefits to both accuracy and RT. To investigate, we effectively removed mechanisms from the model by changing parameter values. To the extent removing these mechanisms causes miss-fit to observed effects that the model previously fitted, those mechanisms allowed the model to fit the effects. To remove excitation from the model, accumulation rates to decisions in line with the decision aid advice  were set to the values estimated from matched manual trials. Similarly, to remove inhibition, accumulation rates to decisions disagreeing with the decision aid were set equal to their value in matched manual trials. To remove threshold effects, thresholds were set to the levels observed in manual conditions. 

Figures 10 and 11 depict the results of our posterior exploration. We found major miss-fits to automation’s effects on accuracy when inhibition was removed for both high and low reliability conditions, and also major miss-fits to accuracy effects for the high-reliability condition when excitation was removed. This suggests that inhibition was important for explaining automation’s effects on accuracy in both conditions, and excitation important in the high-reliability condition. We also found that removing inhibition from the model caused major miss-fits to the effects of automation on RT for both high and low reliability conditions, particularly costs of incorrect automation to RT, demonstrating the importance of the inhibition mechanism in driving this cost. In contrast to these findings, setting thresholds to manual levels had little effect on predictions regarding automation’s effect, suggesting they were relatively less important to model predictions.   

```{r echo= FALSE}
postexp_accs_cap <- "Figure 8. Exploration of the importance of model mechanisms in explaining automation’s effects on accuracy. Accuracy benefit refers to improvement of accuracy on automation-correct trials, and accuracy cost to decrements in accuracy on automation-incorrect trials. Model mechanisms were removed by setting parameter values equal to matched manual conditions and resulting miss-fit indicates the degree to which that mechanism was responsible for the full model’s ability to predict effects. Model predictions correspond to the white circles, the posterior means correspond to the black shaded dots. The error bars display the 95% posterior credible intervals of the predictions."
```

```{r echo= FALSE , results = "asis", message=FALSE, warning=FALSE, fig.width = 8, fig.height=7, fig.cap=postexp_accs_cap}


combined %>% ggplot(aes(y=mean, x=eff)) + geom_point(size=2)+ 
  geom_errorbar(aes(ymax = upper, ymin = lower), width= 0.2) +
  geom_point(aes_string(x = 'eff', y= 'data'), pch=21, size=3, colour="black")+
 geom_line(aes(group=1,y=data), linetype=2) +facet_grid(model~Auto) +xlab("")+
  ylab("") +
    theme(text = element_text(size = 14))

```


```{r echo= FALSE}
postexp_RTs_cap <- "Figure 9. *Exploration of the importance of model mechanisms in explaining automation’s effects on RT. RT benefit refers to the speeding of correct RT on trials where automation was correct, and RT cost refers to the slowing of RT on trials where the automation was incorrect. Model mechanisms were removed by setting parameter values equal to matched manual conditions and resulting miss-fit indicates the degree to which that mechanism was responsible for the full model’s ability to predict effects. Model predictions correspond to the white circles, the posterior means correspond to the black shaded dots. The error bars display the 95% posterior credible intervals of the predictions.*"
```

```{r echo= FALSE , results = "asis", message=FALSE, warning=FALSE, fig.width = 8, fig.height=7, fig.cap=postexp_RTs_cap}

combined_RT %>% ggplot(aes(y=mean, x=eff)) + geom_point(size=2)+ 
  geom_errorbar(aes(ymax = upper, ymin = lower), width= 0.2) +
  geom_point(aes_string(x = 'eff', y= 'data'), pch=21, size=3, colour="black")+
  geom_line(aes(group=1,y=data), linetype=2) +facet_grid(model~Auto) +xlab("")+
  ylab("")+
    theme(text = element_text(size = 14))

```


# Individual differences

We explored how individual differences in model parameters related to the costs and benefits of automation to accuracy. To do so, we constructed distributions of “plausible value” correlations (Ly et al., 2017), in which a correlation between model parameters and the data of interest is calculated for every posterior sample, creating a distribution of correlations. We then adjusted this plausible value distribution to correct for population-level inference (Ly et al., 2018). This approach of correcting for sample size is quite conservative, and so we are confident that trends identified by this method are robust. In Table 3 we report posterior mean values of these correlations and accompanying 95% credible intervals. In text we refer to “plausible” correlations as those with 95% credible intervals not overlapping 0. For our correlations we obtained a single estimate of excitation and inhibition for each participant by averaging over conflict and non-conflict trials, and over automation-correct and automation-incorrect trials. 

Table 4.
*Plausible value correlations between model mechanisms, benefits of automation, costs of automation, and trust in automation. We constructed distributions of "plausible values" (Ly et al., 2017) of correlations by calculating the correlation across participants between model parameters and dependent variables for every posterior sample.  This was then corrected for population-level inference (Ly et al., 2018). We report posterior mean (95% credible interval) of these correlation distributions. We obtained a single estimate of excitation and inhibition for each participant by averaging over conflict and non-conflict trials, and over automation-correct and automation-incorrect trials*

```{r echo= FALSE , results = "asis", message=FALSE, warning=FALSE, fig.width = 8, fig.height=6}




MCIs <- function(MCI){
  MCI <- round(MCI, 2)
  paste0(MCI["M"], " (", MCI["LCI"], " - ", MCI["HCI"], ")")
  
}

cortab <- rbind(
  c("", ""),
  c(MCIs(ex_L_benefit), MCIs(inh_L_benefit)),
  c(MCIs(ex_L_cost), MCIs(inh_L_cost)),
  c(MCIs(ex_L_benefit_RT), MCIs(inh_L_benefit_RT)),
  c(MCIs(ex_L_cost_RT), MCIs(inh_L_cost_RT)), 
  c(MCIs(ex_L_trust), MCIs(inh_L_trust)), 
  c("", ""),
  c(MCIs(ex_H_benefit), MCIs(inh_H_benefit)),
  c(MCIs(ex_H_cost), MCIs(inh_H_cost)),
  c(MCIs(ex_H_benefit_RT), MCIs(inh_H_benefit_RT)),
  c(MCIs(ex_H_cost_RT), MCIs(inh_H_cost_RT)),
  c(MCIs(ex_H_trust), MCIs(inh_H_trust))
)

rownames(cortab) <- c("Low Reliability", "Accuracy Benefit", "Accuracy Cost", "RT Benefit",
                        "RT Cost", "Trust",
                      "High Reliability",
                      "Accuracy Benefit", "Accuracy Cost", "RT Benefit",
                        "RT Cost", "Trust")
colnames(cortab) <- c("Excitation ", "Inhibition ")

pandoc.table(cortab)

pandoc.table(round(msds, 2))

```

In the high-reliability condition, we found a plausible positive correlation between the benefits of correct automation to accuracy and excitation, but we did not find a plausible correlation between automation accuracy benefits and inhibition. In contrast, in the low-reliability condition, there was a plausible positive correlation between the benefits of correct automation and inhibition, but no plausible correlation between accuracy benefits and excitation. Similarly, in the high-reliability condition there was a plausible positive correlation between the costs of incorrect automation to accuracy and excitation but not inhibition, whereas in the low-reliability condition there was a plausible positive correlation between the costs of incorrect automation to inhibition but not excitation. 

We found that benefits to RT on automation-correct trials were (plausibly) positively correlated with excitation for both the high-reliability condition and the low-reliability condition, whereas they were weakly (not plausibly) negatively correlated with inhibition in both the high-reliability condition and the low-reliability condition. In contrast, costs to RT on automation-incorrect trials were (plausibly) positively correlated with inhibition for both the high-reliability and low-reliability conditions, and weakly (not plausibly) negatively correlated with excitation for both the high-reliability and low-reliability conditions. 

We also examined how model parameters correlated with trust in automation. In the high-reliability condition, there was a plausible positive correlation between excitation and trust, but no plausible correlation between inhibition and trust. In the low-reliability condition, there was not a plausible correlation between trust and either excitation or inhibition.


